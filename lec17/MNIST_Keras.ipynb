{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Varianza contra Sesgo\n",
    "\n",
    "Basado en https://colab.research.google.com/github/AviatorMoser/keras-mnist-tutorial/blob/master/MNIST%20in%20Keras.ipynb\n",
    "\n",
    "Autores: Daniel Moser, Xavier Snelgrove, Yash Katariya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importemos los paquetes necesarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "import numpy as np                   # advanced math library\n",
    "import matplotlib.pyplot as plt      # MATLAB like plotting routines\n",
    "import random                        # for generating random numbers\n",
    "\n",
    "from keras.datasets import mnist     # MNIST dataset is included in Keras\n",
    "from keras.models import Sequential  # Model type to be used\n",
    "\n",
    "from keras.layers.core import Dense, Dropout, Activation # Types of layers to be used in our model\n",
    "from keras.utils import np_utils                         # NumPy related tools\n",
    "\n",
    "# Metrics tools\n",
    "from keras.callbacks import TensorBoard\n",
    "from sklearn.metrics import confusion_matrix, classification_report, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carguemos el set de datos MNIST original con 70000 imágenes de tamaño 28x28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The MNIST data is split between 60,000 28 x 28 pixel training images and 10,000 28 x 28 pixel images\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "print(\"X_train shape\", X_train.shape)\n",
    "print(\"y_train shape\", y_train.shape)\n",
    "print(\"X_test shape\", X_test.shape)\n",
    "print(\"y_test shape\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (7,7) # Make the figures a bit bigger\n",
    "\n",
    "for i in range(9):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    num = random.randint(0, len(X_train))\n",
    "    plt.imshow(X_train[num], cmap='gray', interpolation='none')\n",
    "    plt.title(\"Class {}\".format(y_train[num]))\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just a little function for pretty printing a matrix\n",
    "def matprint(mat, fmt=\"g\"):\n",
    "    col_maxes = [max([len((\"{:\"+fmt+\"}\").format(x)) for x in col]) for col in mat.T]\n",
    "    for x in mat:\n",
    "        for i, y in enumerate(x):\n",
    "            print((\"{:\"+str(col_maxes[i])+fmt+\"}\").format(y), end=\"  \")\n",
    "        print(\"\")\n",
    "\n",
    "# now print!        \n",
    "matprint(X_train[num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(60000, 784) # reshape 60,000 28 x 28 matrices into 60,000 784-length vectors.\n",
    "X_test = X_test.reshape(10000, 784)   # reshape 10,000 28 x 28 matrices into 10,000 784-length vectors.\n",
    "\n",
    "X_train = X_train.astype('float32')   # change integers to 32-bit floating point numbers\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "X_train /= 255                        # normalize each value for each pixel for the entire vector for each input\n",
    "X_test /= 255\n",
    "\n",
    "print(\"Training matrix shape\", X_train.shape)\n",
    "print(\"Testing matrix shape\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding of the output class\n",
    "\n",
    "nb_classes = 10 # number of unique digits\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Sequential model is a linear stack of layers and is very common.\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# FIRST LAYER\n",
    "\n",
    "# The first hidden layer is a set of 512 nodes (artificial neurons).\n",
    "# Each node will receive an element from each input vector and apply some weight and bias to it.\n",
    "\n",
    "model.add(Dense(512, input_shape=(784,))) #(784,) is not a typo -- that represents a 784 length vector!\n",
    "\n",
    "# An \"activation\" is a non-linear function applied to the output of the layer above.\n",
    "# It checks the new value of the node, and decides whether that artifical neuron has fired.\n",
    "# The Rectified Linear Unit (ReLU) converts all negative inputs to nodes in the next layer to be zero.\n",
    "# Those inputs are then not considered to be fired.\n",
    "# Positive values of a node are unchanged.\n",
    "\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# Dropout zeroes a selection of random outputs (i.e., disables their activation)\n",
    "# Dropout helps protect the model from memorizing or \"overfitting\" the training data.\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# SECOND LAYER\n",
    "\n",
    "# The second hidden layer appears identical to our first layer.\n",
    "# However, instead of each of the 512-node receiving 784-inputs from the input image data,\n",
    "# they receive 512 inputs from the output of the first 512-node layer.\n",
    "\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# OUTPUT LAYER\n",
    "\n",
    "# The final layer of 10 neurons in fully-connected to the previous 512-node layer.\n",
    "# The final layer of a FCN should be equal to the number of desired classes (10 in this case).\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# Summarize the built model\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the model\n",
    "\n",
    "Keras is built on top of TensorFlow. Both packages allow you to define a computation graph in Python, which then compiles and runs efficiently on the CPU or GPU without the overhead of the Python interpreter.\n",
    "\n",
    "When compiling a model, Keras asks you to specify your loss function and your optimizer. The loss function we'll use here is called categorical cross-entropy, and is a loss function well-suited to comparing two probability distributions.\n",
    "\n",
    "Our predictions are probability distributions across the ten different digits (e.g. \"we're 80% confident this image is a 3, 10% sure it's an 8, 5% it's a 2, etc.\"), and the target is a probability distribution with 100% for the correct category, and 0 for everything else. The cross-entropy is a measure of how different your predicted distribution is from the target distribution. More detail at Wikipedia\n",
    "\n",
    "The optimizer helps determine how quickly the model learns through gradient descent. The rate at which descends a gradient is called the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the Adam optimizer for learning\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TensorBoard Callback\n",
    "tcb = TensorBoard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "m=min(10000,np.shape(X_train)[0])\n",
    "history_callback = model.fit(X_train[:m,:], Y_train[:m,:],\n",
    "                             batch_size=128, epochs=5,\n",
    "                             verbose=0,\n",
    "                             validation_data=(X_test, Y_test),\n",
    "                             callbacks=[tcb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, Y_test)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history_callback.history['accuracy']\n",
    "loss = history_callback.history['loss']\n",
    "\n",
    "val_acc = history_callback.history['val_accuracy']\n",
    "val_loss = history_callback.history['val_loss']\n",
    "\n",
    "fig, (ax1,ax2) = plt.subplots(2,figsize=(8,6))\n",
    "ax1.plot(acc,label=\"Training Accuracy\")\n",
    "ax1.plot(val_acc,label=\"Validation Accuracy\")\n",
    "ax2.plot(loss,label=\"Training Loss\")\n",
    "ax2.plot(val_loss,label=\"Validation Loss\")\n",
    "\n",
    "ax2.set_xlabel('epochs')\n",
    "ax1.legend()\n",
    "ax2.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test data \n",
    "pY_test = model.predict(X_test)\n",
    "\n",
    "# Both the ground-truth Y_test and the preditions pY_test \n",
    "# are one-hot encoded and we need a categorial encoding for\n",
    "# the confusion matrix.  argmax comes to the rescue.\n",
    "groundTruth = y_test # Y_test.argmax(axis=1)\n",
    "predictions = pY_test.argmax(axis=1)\n",
    "\n",
    "\n",
    "cm = confusion_matrix(groundTruth, predictions)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp = classification_report(groundTruth, predictions)\n",
    "print(rp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = precision_recall_fscore_support(groundTruth, predictions,average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Precision={0}, Recall={1}\".format(stats[0],stats[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
