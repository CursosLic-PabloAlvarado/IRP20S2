{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frozen Lake\n",
    "\n",
    "Frozen Lake is one of the environments in gym, ideal to play around with value and policy iteration:\n",
    "\n",
    "https://gym.openai.com/envs/FrozenLake-v0/\n",
    "\n",
    "The following code is based on [Justin Francis](https://github.com/wagonhelm/Value-Iteration)' solution.\n",
    "\n",
    "You might need to install gym\n",
    "\n",
    "`pip3 install -U gym`\n",
    "\n",
    "or \n",
    "\n",
    "`conda install -c conda-forge gym`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake8x8-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "print(\"States =\",n_states,\"; Actions =\",n_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implemented By Using Pseudo Code From: <br />\n",
    "[Reinforcement Learning: An Introduction](http://www.incompleteideas.net/sutton/book/the-book-2nd.html) <br />\n",
    "By: Richard Sutton and Andrew Barto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/value.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = np.zeros(n_states)\n",
    "gamma = 0.999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(1,1,1,projection='3d')\n",
    "x = np.outer(np.linspace(1,8,8), np.ones(8))\n",
    "y = x.copy().T\n",
    "surf=None\n",
    "\n",
    "while True:\n",
    "    delta = 0\n",
    "    for state in reversed(range(n_states)):\n",
    "        v = values[state]\n",
    "        values[state] = np.max([sum([p*(r + gamma*values[s_]) for p, s_, r, _ in env.env.P[state][a]]) for a in range(env.env.nA)])\n",
    "        delta = max(delta,abs(v-values[state]))\n",
    "\n",
    "    # Show the found values\n",
    "    if surf:\n",
    "        surf.remove()\n",
    "    z = values.reshape(8,8)\n",
    "    surf=ax.plot_surface(x, y, z, cmap='seismic', rstride=1, cstride=1, linewidth=0, alpha=0.7)\n",
    "    fig.canvas.draw()\n",
    "    \n",
    "    # Stop if too small the change\n",
    "    if delta < 1e-3:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []\n",
    "bestAverage = []\n",
    "state = env.reset()\n",
    "test_episodes = 1000\n",
    "\n",
    "for i in range(1,test_episodes):\n",
    "    while True:\n",
    "        action = np.argmax([sum([p*(r + gamma*values[s_]) for p, s_, r, _ in env.env.P[state][a]]) for a in range(env.env.nA)])\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        if done:\n",
    "            clear_output(wait=True)\n",
    "            env.render()\n",
    "\n",
    "            history.append(reward)\n",
    "            env.reset()           \n",
    "            \n",
    "            break\n",
    "        \n",
    "    if len(history)>=100 and np.mean(history[i-100:i]) >= 0.30:\n",
    "        bestAverage.append(np.mean(history[i-100:i]))\n",
    "        \n",
    "print('Best Average Score Per 100 Episodes in {} Episodes: {:4.3f}'.format(test_episodes, np.max(bestAverage)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value Iteration assumes that the entire MDP is known but in most cases the MDP is unknown.  Let's try and solve FrozenLake-v0 by trying to estimate all p(s',r|s,a) and state values and by using e-greedy Q-Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = env.observation_space.n\n",
    "n_actions = env.action_space.n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also think of our states like the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = np.zeros([n_states])\n",
    "for i in range(n_states):\n",
    "    states[i] = i\n",
    "states.reshape(4,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 4 possible actions with their associated labels\n",
    "\n",
    "[0] = Left <br />\n",
    "[1] = Down <br />\n",
    "[2] = Right <br />\n",
    "[3] = Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = np.zeros([n_states,n_states,n_actions,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table represents the number of initial states (**s**), future states (**s'**), number of possible actions(**a**), and the last 4 indexes represent:\n",
    "\n",
    "[0] - Reward<br />\n",
    "[1] - How many times action is taken given state (**#a|s**)<br />\n",
    "[2] - How many times state transitioned to **s'** given **a** (**#s'|a**)<br />\n",
    "[3] - Probability of transitioning to **s'** from **s** given action **a** (**p(s'|s,a)**)<br />\n",
    "\n",
    "Since the rewards for state transitions do not change we do not need to calculate **p(s',r|s,a**)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a function that can evaluate all possible state transitions for any given state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def possibleStates(s):\n",
    "    p_states = []\n",
    "    for i in range(n_states):\n",
    "        if np.max(table[s,i,:,[3]]):\n",
    "            p_states.append(i)\n",
    "    return p_states\n",
    "\n",
    "def actionProbs(s):\n",
    "    for i in possibleStates(s):\n",
    "        print('-----------------------------------------------------')\n",
    "        for a in range(n_actions):\n",
    "            print('P: State: [{}] -> State: [{}] via Action: [{}] is {:4.4f}'.format(s,i,a,table[s,i,a,3]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the probability of each state transition from state[0] by sampling all possible actions from the initial state 100,000 times.  By [law of large numbers](https://www.youtube.com/watch?v=Ws63I3F7Moc) we will converge to the state transition probabilities.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_actions):\n",
    "    for _ in range(10000):\n",
    "        state = env.reset()\n",
    "        action = i\n",
    "        state2, reward, done, info = env.step(action)\n",
    "\n",
    "        table[state,state2,action,0] = reward\n",
    "        table[state,:,action,1] += 1\n",
    "        table[state,state2,action,2] += 1\n",
    "        table[state,:,action,3] = table[state,:,action,2] / table[state,:,action,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can display the probability of transitioning to any possible future state given any action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actionProbs(env.reset())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if we wanted to transition to state 4, the best possible action is [0] (LEFT) because from state 0 we will have a 66% chance of staying in state 0 and a 33% chance of transitioning to state 4.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to solve this environment we will have to sample it a large amount of times and keep track of the estimated value of each state.  We will create a value table of 16 for each possible states.  We update our state value using the TD Target also known as Tabular TD(0) for estimating V(pi) the optimal values.  We will use learning rate alpha to determine the rate of update we apply to V(s).  The value can be considered our expected return in a given state.  We will explore the environment by behaving completely randomly updating the values at the same time which will be used later for another policy for completeing the environment.  This is also known as off-policy learning since we are not adapting the policy we are using to explore the environment.  Since it's exploring randomly it sometimes does not converge nicely without a very large amount of episodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = np.zeros([n_states])\n",
    "table = np.zeros([n_states,n_states,n_actions,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.999\n",
    "alpha = 0.1\n",
    "history = []\n",
    "state = env.reset()\n",
    "n_episodes = 500000\n",
    "n_steps = 500\n",
    "n_solved = 0\n",
    "for i in range(n_episodes):\n",
    "    state = env.reset()\n",
    "    for _ in range(n_steps):\n",
    "        action = env.action_space.sample()\n",
    "        state2, reward, done, info = env.step(action)\n",
    "\n",
    "        table[state,state2,action,0] = reward\n",
    "        table[state,:,action,1] += 1\n",
    "        table[state,state2,action,2] += 1\n",
    "        table[state,:,action,3] = table[state,:,action,2] / table[state,:,action,1]\n",
    "        \n",
    "        value[state] += alpha*(reward + gamma*value[state2] - value[state])\n",
    "        \n",
    "        state = state2\n",
    "        if done:\n",
    "            if reward == 1:\n",
    "                n_solved += 1\n",
    "            break\n",
    "              \n",
    "print('Solved {} Times In {} Episodes with average reward of {}'.format(n_solved, i+1, n_solved/n_episodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value2 = value.reshape(4,4)\n",
    "for i in range(4):\n",
    "    print(value2[i,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the holes and and goal point do not have any values because they do not lead to any future states.  When trying to estimate the best action to take we will take the probabilities of of all actions that lead to the best state and subtract them by the action probabilities for the worst state.  This ensures that actions that have a probability of leading to a worse state are not considered.  Since the holes and and finish point do not have values we will assign their values to be the expected reward for transitioning to that state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bestState(s):\n",
    "    beststate = s\n",
    "    for i in possibleStates(s):\n",
    "        if value[i] + np.max(table[:,i,:,0]) > value[beststate]:\n",
    "            beststate = i\n",
    "    return beststate\n",
    "\n",
    "def worstState(s):\n",
    "    worststate = s\n",
    "    for i in possibleStates(s):\n",
    "        if value[i] + np.max(table[:,i,:,0]) < value[worststate]:\n",
    "            worststate = i\n",
    "    return worststate\n",
    "\n",
    "def bestAction(s):\n",
    "    a = np.argmax((table[s,bestState(s),:,[3]])-(table[s,worstState(s),:,[3]]))\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []\n",
    "for i in range(1,1000):\n",
    "    while True:\n",
    "        action = bestAction(state)\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        if done:\n",
    "            history.append(reward)\n",
    "            env.reset()\n",
    "            break\n",
    "\n",
    "    # Open AI's Standards For Solving\n",
    "    if len(history)>=100 and np.mean(history[i-100:i]) >= 0.78:\n",
    "        print('Solved in {} attempts'.format(i))\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There we have it by only behaving completely random and updating our expected state values we are able to solve the environment.  Here is the graph of the estimated state values. [1,1] represents the start state and [4,4] the finish state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = plt.axes(projection='3d')\n",
    "x = np.outer(np.linspace(1, 4, 4), np.ones(4))\n",
    "y = x.copy().T\n",
    "z = value2\n",
    "ax.plot_surface(x, y, z, cmap='seismic', rstride=1, cstride=1, linewidth=0, alpha=0.7)\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also here is some old code I used for solving this environment using e-greedy Q-Learning (Action Value), which is a hell of alot less complicated then my previous method.  This e-greedy method also incorporates some random action noise which decays over each episode.  This method is on-policy and solves much faster because it's improving the Q-Value based policy every time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/qlearn.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def e_greedy(eps, Q, state, episode):\n",
    "    \n",
    "    if np.random.rand() > eps:\n",
    "        action = np.argmax(Q[state,:]+np.random.randn(1, n_actions)/(episode/4))\n",
    "    else:\n",
    "        action = env.action_space.sample()\n",
    "        eps -= 10**-5\n",
    "\n",
    "    return action, eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "rewardTracker = []\n",
    "\n",
    "alpha = 0.8\n",
    "gamma = 0.95\n",
    "eps = 0.1\n",
    "numTrainingEpisodes = 5000\n",
    "numTrainingSteps = 300\n",
    "\n",
    "for episode in range(1,numTrainingEpisodes+1):  \n",
    "\n",
    "    G = 0\n",
    "    state = env.reset()\n",
    "\n",
    "    for step in range(1,numTrainingSteps):\n",
    "\n",
    "        action, eps = e_greedy(eps, Q, state, episode)\n",
    "        state2, reward, done, info = env.step(action)\n",
    "        Q[state,action] += alpha * (reward + gamma * np.max(Q[state2]) - Q[state,action])\n",
    "        state = state2\n",
    "        G += reward\n",
    "\n",
    "    rewardTracker.append(G)\n",
    "\n",
    "    if episode % (numTrainingEpisodes*.10) == 0 and episode != 0:\n",
    "        print('Alpha {}  Gamma {}  Epsilon {:04.3f}  Episode {} of {}'.format(alpha, gamma, eps, episode, numTrainingEpisodes))\n",
    "        print(\"Average Total Return: {}\".format(sum(rewardTracker)/episode))\n",
    "\n",
    "    if (sum(rewardTracker[episode-100:episode])/100.0) > .78:\n",
    "        print('-------------------------------------------------------')\n",
    "        print('Solved after {} episodes with average return of {}'.format(episode-100, sum(rewardTracker[episode-100:episode])/100.0))\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the graphs for all the action values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(221,projection='3d')\n",
    "x = np.outer(np.linspace(1, 4, 4), np.ones(4))\n",
    "y = x.copy().T\n",
    "\n",
    "ax.plot_surface(x, y, Q[:,0].reshape(4,4), cmap='magma', rstride=1, cstride=1, linewidth=0, alpha=0.7)\n",
    "plt.title('Action[0] Left')\n",
    "\n",
    "ax = fig.add_subplot(222,projection='3d')\n",
    "ax.plot_surface(x, y, Q[:,1].reshape(4,4), cmap='magma', rstride=1, cstride=1, linewidth=1, alpha=0.7)\n",
    "plt.title('Action[1] Down')\n",
    "\n",
    "ax = fig.add_subplot(223,projection='3d')\n",
    "ax.plot_surface(x, y, Q[:,2].reshape(4,4), cmap='magma', rstride=1, cstride=1, linewidth=1, alpha=0.7)\n",
    "plt.title('Action[2] Right')\n",
    "\n",
    "ax = fig.add_subplot(224,projection='3d')\n",
    "ax.plot_surface(x, y, Q[:,3].reshape(4,4), cmap='magma', rstride=1, cstride=1, linewidth=1, alpha=0.7)\n",
    "plt.title('Action[3] Up')\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
