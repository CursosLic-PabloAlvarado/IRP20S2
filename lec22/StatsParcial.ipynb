{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EL4703 Señales y Sistemas 2020 S2\n",
    "## Estadísticas del examen parcial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "from scipy.special import erf\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuración:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archivo_csv = 'parcial_all.csv' ## Recordar que las columnas tienen que estar separadas por ';'\n",
    "\n",
    "col_pts = 1   ## Columna en el CSV con los puntos a analizar\n",
    "total_pts = 46 ## Máximo puntaje obtenible en la columna col_pts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargar todos los datos en archivo csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.genfromtxt(archivo_csv, delimiter=';')\n",
    "print(\"Datos completos en matriz {0} x {1}\".format(data.shape[0],data.shape[1]))\n",
    "\n",
    "# La columna col_pts tiene los puntos totales obtenidos por cada estudiante\n",
    "pts=data[:,col_pts].reshape(-1,1)\n",
    "print(pts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostrar la densidad probabilistica estimada para un ancho de banda dado manualmente\n",
    "xplot = np.linspace(0,total_pts,200)[:,np.newaxis]\n",
    "kde = KernelDensity(kernel=\"gaussian\",bandwidth=3).fit(pts)\n",
    "log_dens = kde.score_samples(xplot)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(xplot[:,0],np.exp(log_dens))\n",
    "plt.xlabel(\"Puntos\")\n",
    "plt.ylabel(\"p(Puntos)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimar el ancho de banda óptimo para estos puntos\n",
    "params = {'bandwidth': np.logspace(-1,1,200)}\n",
    "grid = GridSearchCV(KernelDensity(),params,cv=10) # Use 10-fold cross-validation\n",
    "grid.fit(pts)\n",
    "kde = grid.best_estimator_\n",
    "print(\"Mejor ancho de banda: {0}\".format(kde.bandwidth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muestre la densidad estimada con el mejor ancho de banda\n",
    "log_dens = kde.score_samples(xplot)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(xplot[:,0],np.exp(log_dens))\n",
    "plt.xlabel(\"Puntos\")\n",
    "plt.ylabel(\"p(Puntos)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GMM\n",
    "\n",
    "Si usamos solo los datos originales, como son relativamente pocos, el GMM queda lejos de la densidad estimada con el kernel gaussiano, por lo que mejor muestreamos el proceso con la densisdad estimada y sacamos muchos más datos, y a partir de esos estimamos el GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = GaussianMixture(n_components=2,covariance_type=\"spherical\",verbose=2)\n",
    "\n",
    "samples = kde.sample(50000) # Usemos MUCHOS datos\n",
    "gmm.fit(samples)\n",
    "print(\"Weights: \",gmm.weights_)\n",
    "print(\"Means: \",gmm.means_)\n",
    "print(\"Variances: \", gmm.covariances_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dens = (gmm.weights_[0]*norm(gmm.means_[0],np.sqrt(gmm.covariances_[0])).pdf(xplot[:,0])\n",
    "             + gmm.weights_[1]*norm(gmm.means_[1],np.sqrt(gmm.covariances_[0])).pdf(xplot[:,0]))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(xplot[:,0],np.exp(log_dens),'-b',label=\"KDE\")\n",
    "plt.plot(xplot[:,0],pred_dens,'-k',label=\"GMM\",linewidth=3)\n",
    "plt.plot(xplot[:,0],gmm.weights_[0]*norm.pdf(xplot[:,0],gmm.means_[0],np.sqrt(gmm.covariances_[0])),'-r')\n",
    "plt.plot(xplot[:,0],gmm.weights_[1]*norm.pdf(xplot[:,0],gmm.means_[1],np.sqrt(gmm.covariances_[1])),'-g')\n",
    "\n",
    "plt.xlabel(\"Puntos\")\n",
    "plt.ylabel(\"p(Puntos)\")\n",
    "plt.legend(loc='upper left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el experimento anterior definimos a priori que queríamos dos gaussianas en el GMM, pero ¿será mejor otro modelo?.  Lo siguiente busca exhaustivamente el número óptimo de gaussianas a utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowest_bic = np.infty\n",
    "best_n_components = -1\n",
    "bic = []\n",
    "\n",
    "n_components_range = range(1, 7)\n",
    "cv_type = 'spherical'\n",
    "for n_components in n_components_range:\n",
    "    # Fit a Gaussian mixture with EM\n",
    "    gm = GaussianMixture(n_components=n_components,covariance_type=cv_type)\n",
    "    gm.fit(samples)\n",
    "    bic.append(gm.bic(samples)) # Bayesian Information Criterion\n",
    "    #bic.append(gm.aic(samples)) # Akaike Information Criterion\n",
    "    print(\"  bic con {0} componentes = {1}\".format(n_components,bic[-1]))\n",
    "    if bic[-1] < lowest_bic:\n",
    "        lowest_bic = bic[-1]\n",
    "        best_gmm = gm\n",
    "        best_n_components = n_components\n",
    "        \n",
    "print(\"Mejor número de componentes en GMM: \",best_n_components)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def Phi(z):\n",
    "    return 0.5*(1 + erf(z/np.sqrt(2)))\n",
    "\n",
    "t=np.linspace(0,total_pts,1000)[:,np.newaxis]\n",
    "\n",
    "f=np.argmin(gmm.means_)\n",
    "s=np.argmax(gmm.means_)\n",
    "\n",
    "k=np.sqrt(np.pi)/2\n",
    "f0=gmm.weights_[f]*Phi((-t+gmm.means_[f])/np.sqrt(gmm.covariances_[f]))\n",
    "f1=gmm.weights_[s]*Phi((t-gmm.means_[s])/np.sqrt(gmm.covariances_[s]))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(t,f0,label=\"left gaussian error\")\n",
    "plt.plot(t,f1,label=\"right gaussian error\")\n",
    "plt.plot(t,f0+f1,label=\"classification error\")\n",
    "plt.xlabel(\"Decision Threshold\")\n",
    "plt.ylabel(\"Error probability\")\n",
    "plt.legend(loc='upper center')\n",
    "\n",
    "idx = np.argwhere(np.diff(np.sign(f1 - f0).flatten('F')))\n",
    "threshold = t.item(int(idx))\n",
    "clferror = f0.item(int(idx))+f1.item(int(idx))\n",
    "print(\"Minimal classification error at {0} pts -> {1}% = {2}\".format(threshold,100*threshold/total_pts,clferror))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(pts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
